{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An advanced Twitter scraping & OSINT tool written in Python that doesn't use Twitter's API.\n",
    "import twint\n",
    "\n",
    "# Solve compatibility issues with notebooks and RunTime errors.\n",
    "import nest_asyncio\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"twint/\")\n",
    "nest_asyncio.apply()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Python preprocessing library.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable             Type       Description\n",
    "# Username             (string) - Twitter user's username\n",
    "# User_id              (string) - Twitter user's user_id\n",
    "# Search               (string) - Search terms\n",
    "# Geo                  (string) - Geo coordinates (lat,lon,km/mi.)\n",
    "# Location             (bool)   - Set to True to attempt to grab a Twitter user's location (slow).\n",
    "# Near                 (string) - Near a certain City (Example: london)\n",
    "# Lang                 (string) - Compatible language codes: https://github.com/twintproject/twint/wiki/Langauge-codes\n",
    "# Output               (string) - Name of the output file.\n",
    "# Elasticsearch        (string) - Elasticsearch instance\n",
    "# Timedelta            (int)    - Time interval for every request (days)\n",
    "# Year                 (string) - Filter Tweets before the specified year.\n",
    "# Since                (string) - Filter Tweets sent since date (Example: 2017-12-27).\n",
    "# Until                (string) - Filter Tweets sent until date (Example: 2017-12-27).\n",
    "# Email                (bool)   - Set to True to show Tweets that _might_ contain emails.\n",
    "# Phone                (bool)   - Set to True to show Tweets that _might_ contain phone numbers.\n",
    "# Verified             (bool)   - Set to True to only show Tweets by _verified_ users\n",
    "# Store_csv            (bool)   - Set to True to write as a csv file.\n",
    "# Store_json           (bool)   - Set to True to write as a json file.\n",
    "# Custom               (dict)   - Custom csv/json formatting (see below).\n",
    "# Show_hashtags        (bool)   - Set to True to show hashtags in the terminal output.\n",
    "# Limit                (int)    - Number of Tweets to pull (Increments of 20).\n",
    "# Count                (bool)   - Count the total number of Tweets fetched.\n",
    "# Stats                (bool)   - Set to True to show Tweet stats in the terminal output.\n",
    "# Database             (string) - Store Tweets in a sqlite3 database. Set this to the DB. (Example: twitter.db)\n",
    "# To                   (string) - Display Tweets tweeted _to_ the specified user.\n",
    "# All                  (string) - Display all Tweets associated with the mentioned user.\n",
    "# Debug                (bool)   - Store information in debug logs.\n",
    "# Format               (string) - Custom terminal output formatting.\n",
    "# Essid                (string) - Elasticsearch session ID.\n",
    "# User_full            (bool)   - Set to True to display full user information. By default, only usernames are shown.\n",
    "# Profile_full         (bool)   - Set to True to use a slow, but effective method to enumerate a user's Timeline.\n",
    "# Store_object         (bool)   - Store tweets/user infos/usernames in JSON objects.\n",
    "# Store_pandas         (bool)   - Save Tweets in a DataFrame (Pandas) file.\n",
    "# Pandas_type          (string) - Specify HDF5 or Pickle (HDF5 as default).\n",
    "# Pandas               (bool)   - Enable Pandas integration.\n",
    "# Index_tweets         (string) - Custom Elasticsearch Index name for Tweets (default: twinttweets).\n",
    "# Index_follow         (string) - Custom Elasticsearch Index name for Follows (default: twintgraph).\n",
    "# Index_users          (string) - Custom Elasticsearch Index name for Users (default: twintuser).\n",
    "# Index_type           (string) - Custom Elasticsearch Document type (default: items).\n",
    "# Retries_count        (int)    - Number of retries of requests (default: 10).\n",
    "# Resume               (int)    - Resume from a specific tweet id (**currently broken, January 11, 2019**).\n",
    "# Images               (bool)   - Display only Tweets with images.\n",
    "# Videos               (bool)   - Display only Tweets with videos.\n",
    "# Media                (bool)   - Display Tweets with only images or videos.\n",
    "# Replies              (bool)   - Display replies to a subject.\n",
    "# Pandas_clean         (bool)   - Automatically clean Pandas dataframe at every scrape.\n",
    "# Lowercase            (bool)   - Automatically convert uppercases in lowercases.\n",
    "# Pandas_au            (bool)   - Automatically update the Pandas dataframe at every scrape.\n",
    "# Proxy_host           (string) - Proxy hostname or IP.\n",
    "# Proxy_port           (int)    - Proxy port.\n",
    "# Proxy_type           (string) - Proxy type.\n",
    "# Tor_control_port     (int) - Tor control port.\n",
    "# Tor_control_password (string) - Tor control password (not hashed).\n",
    "# Retweets             (bool)   - Display replies to a subject.\n",
    "# Hide_output          (bool)   - Hide output.\n",
    "# Get_replies          (bool)   - All replies to the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "\n",
    "# 63 Top Forex Twitter Accounts: \n",
    "# https://www.forexcrunch.com/60-top-forex-twitter-accounts/\n",
    "# https://towardsdatascience.com/analyzing-tweets-with-nlp-in-minutes-with-spark-optimus-and-twint-a0c96084995f\n",
    "def tweets_dateframe(search, output_file, year=\"2020\"):\n",
    "    # Configure\n",
    "    c = twint.Config()\n",
    "    c.Search = search\n",
    "    c.Year = year\n",
    "    c.Lang = \"en\"\n",
    "    c.Pandas = True\n",
    "    c.Store_csv = True\n",
    "    c.Format = \"Username: {username} |  Tweet: {tweet}\"\n",
    "    c.Output = output_file\n",
    "    c.Hide_output = True\n",
    "    # c.Limit = 10000\n",
    "    # c.User_full = True\n",
    "    # c.Since = since\n",
    "    # c.Until = until\n",
    "\n",
    "    # Run\n",
    "    with HiddenPrints():\n",
    "        print(twint.run.Search(c))\n",
    "    \n",
    "    return \"Done scraping tweets!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'user-agent': \n",
    "           'Mozilla/5.0 (Macintosh Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'}\n",
    "url = \"https://www.forexcrunch.com/60-top-forex-twitter-accounts/\"\n",
    "res = requests.get(url, headers=headers).text\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "trader_account = []\n",
    "table = soup.find(name=\"ol\")\n",
    "for account in table.find_all(name=\"li\"):\n",
    "    name = account.find(name=\"a\").text\n",
    "    name = name.replace(\"@\", \"\")\n",
    "    trader_account.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twint Proxies Issues (Not Solved)\n",
    "Twitter is blocking your IP for a fixed period of time (about 7 mins) after scraping a large batch of tweets. This progressive backoff is the best work around we've found so far. You'll just have to suck it up unless you can rotate your IP each time you get blocked.\n",
    "\n",
    "Reference from [here](https://github.com/twintproject/twint/issues/604)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# for year in tqdm([\"2017\", \"2018\", \"2019\", \"2020\"]):\n",
    "#     for account in trader_account:\n",
    "#         tweets_dateframe(search=account, output_file=\"forex.csv\", year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File forex.csv does not exist: 'forex.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f2b157ae0392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m df = pd.read_csv(\"forex.csv\", \n\u001b[1;32m----> 2\u001b[1;33m                  usecols=[\"date\", \"time\", \"username\", \"tweet\", \"hashtags\", \"likes_count\", \"replies_count\", \"retweets_count\"])\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"# of tweets: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File forex.csv does not exist: 'forex.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"forex.csv\", \n",
    "                 usecols=[\"date\", \"time\", \"username\", \"tweet\", \"hashtags\", \"likes_count\", \"replies_count\", \"retweets_count\"])\n",
    "print(\"# of tweets: {}\".format(df.shape[0]))\n",
    "df.sort_values(by=\"date\", ascending=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(df.isnull(), cbar=True, cmap=sns.color_palette(\"GnBu_d\"))\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = df.shape[0]\n",
    "print(\"# of tweets before dropping duplicates: {}\".format(before))\n",
    "df.drop_duplicates(inplace=True)\n",
    "after = df.shape[0]\n",
    "print(\"# of tweets after dropping duplicates: {}\".format(after))\n",
    "print(\"# of tweets being dropped: {}\".format(before-after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
